# transparent_editor_test.py
"""
Transparent Editor Test - Uses existing production agents without modification

This test:
1. Uses your existing orchestrator and agents unchanged
2. Adds transparency logging around the existing editor
3. Captures and analyzes editor decisions without modifying production code
4. Produces detailed analysis files showing why you get 5 restaurants
"""

import os
import json
import time
import logging
import tempfile
import threading
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class TransparentEditorTest:
    """
    Test editor pipeline with transparency WITHOUT changing production code
    """

    def __init__(self, config, orchestrator):
        self.config = config
        self.orchestrator = orchestrator
        self.admin_chat_id = getattr(config, 'ADMIN_CHAT_ID', None)

        # Use existing production agents unchanged
        self.query_analyzer = orchestrator.query_analyzer
        self.search_agent = orchestrator.search_agent  
        self.scraper = orchestrator.scraper
        self.text_cleaner = orchestrator.text_cleaner
        self.editor_agent = orchestrator.editor_agent  # Use existing editor

        logger.info("‚úÖ Transparent Editor Test initialized with production agents")

    async def test_editor_pipeline(self, restaurant_query: str, bot=None) -> str:
        """
        Test complete editor pipeline with transparency analysis
        """
        total_start_time = time.time()

        # Create files
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        scraped_filename = f"scraped_content_{timestamp}.txt"
        edited_filename = f"edited_results_{timestamp}.txt"

        scraped_filepath = os.path.join(tempfile.gettempdir(), scraped_filename)
        edited_filepath = os.path.join(tempfile.gettempdir(), edited_filename)

        logger.info(f"üß™ Testing Editor Pipeline: {restaurant_query}")

        try:
            # =================================================================
            # STEP 1: QUERY ANALYSIS
            # =================================================================
            logger.info("üîç Step 1: Query Analysis")
            step1_start = time.time()

            query_result = self.query_analyzer.analyze_query(restaurant_query)
            destination = query_result.get('destination', 'Unknown')
            search_queries = query_result.get('english_queries', []) + query_result.get('local_queries', [])

            step1_time = time.time() - step1_start

            # =================================================================
            # STEP 2: WEB SEARCH (bypass database)
            # =================================================================
            logger.info("üåê Step 2: Web Search")
            step2_start = time.time()

            search_results = await self.search_agent.search_restaurants(search_queries, destination)

            step2_time = time.time() - step2_start

            # =================================================================
            # STEP 3: SCRAPING
            # =================================================================
            logger.info("ü§ñ Step 3: Scraping")
            step3_start = time.time()

            scraping_results = await self.scraper.scrape_restaurant_content(
                search_results, destination, max_concurrent=2
            )

            step3_time = time.time() - step3_start

            # =================================================================
            # STEP 4: TEXT CLEANING
            # =================================================================
            logger.info("üßπ Step 4: Text Cleaning")
            step4_start = time.time()

            cleaned_results = []
            for result in scraping_results:
                if result.get('scraped_content'):
                    cleaned_content = await self.text_cleaner.clean_scraped_content(
                        result['scraped_content'], result.get('url', ''), destination
                    )

                    cleaned_result = result.copy()
                    cleaned_result['cleaned_content'] = cleaned_content
                    cleaned_results.append(cleaned_result)

            step4_time = time.time() - step4_start

            # =================================================================
            # STEP 5: SAVE SCRAPED CONTENT
            # =================================================================
            logger.info("üíæ Step 5: Save Scraped Content")
            self._save_scraped_content_analysis(cleaned_results, restaurant_query, destination, scraped_filepath)

            # =================================================================
            # STEP 6: PRE-EDITOR ANALYSIS
            # =================================================================
            logger.info("üîç Step 6: Pre-Editor Analysis")
            pre_analysis = self._analyze_content_before_editor(cleaned_results, destination)

            # =================================================================
            # STEP 7: EDITOR PROCESSING (using existing editor)
            # =================================================================
            logger.info("‚úèÔ∏è Step 7: Editor Processing")
            step7_start = time.time()

            # Prepare for existing editor
            editor_input = []
            for result in cleaned_results:
                if result.get('cleaned_content'):
                    editor_input.append({
                        'url': result.get('url', ''),
                        'scraped_content': result.get('cleaned_content', ''),
                        'title': result.get('title', ''),
                        'domain': result.get('url', '').split('/')[2] if result.get('url') else 'unknown'
                    })

            # Call existing editor (no changes to production code)
            editor_output = self.editor_agent.edit(
                scraped_results=editor_input,
                database_restaurants=None,
                raw_query=restaurant_query,
                destination=destination
            )

            step7_time = time.time() - step7_start

            # =================================================================
            # STEP 8: POST-EDITOR ANALYSIS
            # =================================================================
            logger.info("üìä Step 8: Post-Editor Analysis")

            total_time = time.time() - total_start_time

            self._save_editor_transparency_analysis(
                edited_filepath, restaurant_query, destination, 
                cleaned_results, editor_output, pre_analysis,
                {
                    'step1_time': step1_time,
                    'step2_time': step2_time,
                    'step3_time': step3_time,
                    'step4_time': step4_time,
                    'step7_time': step7_time,
                    'total_time': total_time,
                    'search_results_count': len(search_results),
                    'scraped_results_count': len(scraping_results),
                    'cleaned_results_count': len(cleaned_results),
                    'final_restaurants_count': len(editor_output.get('edited_results', {}).get('main_list', []))
                }
            )

            # =================================================================
            # STEP 9: SEND RESULTS
            # =================================================================
            if bot and self.admin_chat_id:
                await self._send_results_to_admin(
                    bot, scraped_filepath, edited_filepath, restaurant_query, editor_output
                )

            return edited_filepath

        except Exception as e:
            logger.error(f"‚ùå Error in editor pipeline test: {e}")

            # Save error
            with open(edited_filepath, 'w', encoding='utf-8') as f:
                f.write("EDITOR PIPELINE TEST ERROR\n")
                f.write(f"Query: {restaurant_query}\n")
                f.write(f"Error: {str(e)}\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")

                import traceback
                f.write(f"\nTraceback:\n{traceback.format_exc()}\n")

            return edited_filepath

    def _analyze_content_before_editor(self, cleaned_results: List[Dict], destination: str) -> Dict[str, Any]:
        """
        Analyze content BEFORE editor to predict behavior
        """
        total_chars = 0
        restaurant_mentions = 0
        destination_mentions = 0
        source_analysis = []

        destination_keywords = destination.lower().split()
        restaurant_keywords = ['restaurant', 'caf√©', 'bistro', 'bar', 'taverna', 'trattoria']

        for result in cleaned_results:
            content = result.get('cleaned_content', '')
            url = result.get('url', '')
            domain = url.split('/')[2] if '/' in url else 'unknown'

            content_length = len(content)
            total_chars += content_length

            # Count mentions
            content_lower = content.lower()
            source_restaurant_mentions = sum(content_lower.count(keyword) for keyword in restaurant_keywords)
            source_destination_mentions = sum(content_lower.count(word) for word in destination_keywords)

            restaurant_mentions += source_restaurant_mentions
            destination_mentions += source_destination_mentions

            # Quality assessment
            quality_score = min(10, content_length / 1000)
            if source_restaurant_mentions > 0:
                quality_score += 2
            if source_destination_mentions > 0:
                quality_score += 2

            source_analysis.append({
                'domain': domain,
                'content_length': content_length,
                'restaurant_mentions': source_restaurant_mentions,
                'destination_mentions': source_destination_mentions,
                'quality_score': min(quality_score, 10),
                'estimated_restaurants': min(source_restaurant_mentions // 2, 8)
            })

        return {
            'total_sources': len(cleaned_results),
            'total_chars': total_chars,
            'total_restaurant_mentions': restaurant_mentions,
            'total_destination_mentions': destination_mentions,
            'estimated_restaurants': min(restaurant_mentions // 3, 15),
            'avg_quality': sum(s['quality_score'] for s in source_analysis) / len(source_analysis) if source_analysis else 0,
            'geographic_relevance': min(destination_mentions / len(cleaned_results), 10) if cleaned_results else 0,
            'source_analysis': source_analysis
        }

    def _save_scraped_content_analysis(self, cleaned_results: List[Dict], query: str, destination: str, filepath: str):
        """
        Save scraped content with quality analysis
        """
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("SCRAPED CONTENT FOR EDITOR ANALYSIS\n")
                f.write("=" * 80 + "\n")
                f.write(f"Query: {query}\n")
                f.write(f"Destination: {destination}\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Sources: {len(cleaned_results)}\n")
                f.write("=" * 80 + "\n\n")

                # Quality summary
                total_chars = sum(len(r.get('cleaned_content', '')) for r in cleaned_results)
                good_sources = sum(1 for r in cleaned_results if len(r.get('cleaned_content', '')) > 1000)

                f.write("CONTENT QUALITY SUMMARY\n")
                f.write("-" * 40 + "\n")
                f.write(f"Total Content: {total_chars:,} characters\n")
                f.write(f"Good Sources: {good_sources}/{len(cleaned_results)}\n")
                f.write(f"Average per Source: {total_chars // len(cleaned_results):,} chars\n\n")

                # Source breakdown
                f.write("SOURCE BREAKDOWN\n")
                f.write("-" * 40 + "\n")
                for i, result in enumerate(cleaned_results, 1):
                    url = result.get('url', 'Unknown')
                    content = result.get('cleaned_content', '')
                    domain = url.split('/')[2] if '/' in url else 'unknown'

                    f.write(f"{i}. {domain}\n")
                    f.write(f"   Length: {len(content):,} chars\n")
                    f.write(f"   Quality: {'‚úÖ Rich' if len(content) > 2000 else '‚ö†Ô∏è Medium' if len(content) > 500 else '‚ùå Poor'}\n")
                    f.write(f"   URL: {url}\n\n")

                # Full content
                f.write("FULL CONTENT (INPUT TO EDITOR)\n")
                f.write("=" * 80 + "\n\n")

                for i, result in enumerate(cleaned_results, 1):
                    content = result.get('cleaned_content', '')
                    if content and len(content.strip()) > 100:
                        f.write(f"SOURCE {i}\n")
                        f.write("-" * 60 + "\n")
                        f.write(content)
                        f.write("\n" + "=" * 80 + "\n\n")

        except Exception as e:
            logger.error(f"‚ùå Error saving scraped content: {e}")

    def _save_editor_transparency_analysis(
        self, filepath: str, query: str, destination: str,
        cleaned_results: List[Dict], editor_output: Dict[str, Any], 
        pre_analysis: Dict[str, Any], timing_data: Dict[str, Any]
    ):
        """
        Save comprehensive editor decision analysis
        """
        try:
            restaurants = editor_output.get('edited_results', {}).get('main_list', [])

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("EDITOR DECISION TRANSPARENCY ANALYSIS\n")
                f.write("=" * 80 + "\n")
                f.write(f"Query: {query}\n")
                f.write(f"Destination: {destination}\n")
                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Editor: Production EditorAgent (unchanged)\n")
                f.write("=" * 80 + "\n\n")

                # =================================================================
                # INPUT ANALYSIS
                # =================================================================
                f.write("INPUT CONTENT ANALYSIS\n")
                f.write("-" * 40 + "\n")
                f.write(f"Total Sources: {pre_analysis['total_sources']}\n")
                f.write(f"Total Content: {pre_analysis['total_chars']:,} characters\n")
                f.write(f"Restaurant Mentions: {pre_analysis['total_restaurant_mentions']}\n")
                f.write(f"Destination Mentions: {pre_analysis['total_destination_mentions']}\n")
                f.write(f"Estimated Restaurants: {pre_analysis['estimated_restaurants']}\n")
                f.write(f"Average Quality Score: {pre_analysis['avg_quality']:.1f}/10\n")
                f.write(f"Geographic Relevance: {pre_analysis['geographic_relevance']:.1f}/10\n\n")

                # Source details
                f.write("SOURCE-BY-SOURCE PREDICTION:\n")
                for i, source in enumerate(pre_analysis['source_analysis'], 1):
                    f.write(f"  {i}. {source['domain']}\n")
                    f.write(f"     Content: {source['content_length']:,} chars\n")
                    f.write(f"     Restaurant keywords: {source['restaurant_mentions']}\n")
                    f.write(f"     Geographic keywords: {source['destination_mentions']}\n")
                    f.write(f"     Quality: {source['quality_score']:.1f}/10\n")
                    f.write(f"     Expected restaurants: {source['estimated_restaurants']}\n\n")

                # =================================================================
                # EDITOR RESULTS ANALYSIS
                # =================================================================
                f.write("EDITOR RESULTS ANALYSIS\n")
                f.write("-" * 40 + "\n")
                f.write(f"Restaurants Extracted: {len(restaurants)}\n")
                f.write(f"Follow-up Queries: {len(editor_output.get('follow_up_queries', []))}\n\n")

                # Compare prediction vs reality
                expected = pre_analysis['estimated_restaurants']
                actual = len(restaurants)

                f.write("PREDICTION VS REALITY:\n")
                f.write(f"  Predicted: {expected} restaurants\n")
                f.write(f"  Actual: {actual} restaurants\n")
                f.write(f"  Accuracy: {'‚úÖ Good' if abs(expected - actual) <= 2 else '‚ö†Ô∏è Off' if abs(expected - actual) <= 5 else '‚ùå Poor'}\n\n")

                # =================================================================
                # WHY THIS NUMBER OF RESTAURANTS?
                # =================================================================
                f.write("WHY THIS NUMBER OF RESTAURANTS?\n")
                f.write("-" * 40 + "\n")

                if actual == 0:
                    f.write("‚ùå ZERO RESULTS - LIKELY CAUSES:\n")
                    f.write("  ‚Ä¢ Content doesn't contain clear restaurant information\n")
                    f.write("  ‚Ä¢ Geographic mismatch (content about wrong location)\n")
                    f.write("  ‚Ä¢ Paywall/blocked content\n")
                    f.write("  ‚Ä¢ Content quality too low\n\n")

                elif actual < 3:
                    f.write(f"‚ö†Ô∏è LIMITED RESULTS ({actual}) - LIKELY CAUSES:\n")
                    f.write("  ‚Ä¢ Sparse restaurant content in sources\n")
                    f.write("  ‚Ä¢ Editor being selective about quality\n")
                    f.write("  ‚Ä¢ Geographic filtering removing options\n\n")

                elif actual == 5:
                    f.write("üéØ EXACTLY 5 RESULTS - ANALYSIS:\n")
                    f.write("  ‚Ä¢ Sources likely contained ~5 clearly described restaurants\n")
                    f.write("  ‚Ä¢ Editor found sufficient extractable content\n")
                    f.write("  ‚Ä¢ This is NOT a hardcoded limit\n")
                    f.write("  ‚Ä¢ Quality over quantity approach working\n\n")

                elif actual > 7:
                    f.write(f"‚úÖ RICH RESULTS ({actual}) - ANALYSIS:\n")
                    f.write("  ‚Ä¢ High-quality restaurant guide content\n")
                    f.write("  ‚Ä¢ Multiple comprehensive sources\n")
                    f.write("  ‚Ä¢ Editor found abundant extractable information\n\n")

                # =================================================================
                # DETAILED RESTAURANT ANALYSIS
                # =================================================================
                f.write("EXTRACTED RESTAURANTS DETAILS\n")
                f.write("-" * 40 + "\n")

                if restaurants:
                    for i, restaurant in enumerate(restaurants, 1):
                        name = restaurant.get('name', 'Unknown')
                        description = restaurant.get('description', '')
                        cuisine_tags = restaurant.get('cuisine_tags', [])
                        address = restaurant.get('address', '')
                        sources = restaurant.get('sources', [])

                        f.write(f"RESTAURANT {i}: {name}\n")
                        f.write(f"  Cuisine: {', '.join(cuisine_tags[:3]) if cuisine_tags else 'Not specified'}\n")
                        f.write(f"  Address: {address if address else 'Not available'}\n")
                        f.write(f"  Description: {len(description)} characters\n")
                        f.write(f"  Sources: {len(sources)} domains\n")

                        # Quality analysis
                        if description:
                            word_count = len(description.split())
                            f.write(f"  Quality: {'‚úÖ Rich' if word_count > 50 else '‚ö†Ô∏è Basic' if word_count > 20 else '‚ùå Minimal'} ({word_count} words)\n")
                            preview = description[:150] + "..." if len(description) > 150 else description
                            f.write(f"  Preview: {preview}\n")
                        else:
                            f.write(f"  Quality: ‚ùå No description\n")

                        f.write("\n")

                # =================================================================
                # IMPROVEMENT RECOMMENDATIONS
                # =================================================================
                f.write("IMPROVEMENT RECOMMENDATIONS\n")
                f.write("-" * 40 + "\n")

                if actual < 5:
                    f.write("TO GET MORE RESTAURANTS:\n")
                    f.write("  1. Target restaurant-specific sources (food blogs, guides)\n")
                    f.write("  2. Search for 'best restaurants [destination]' format\n")
                    f.write("  3. Include cuisine-specific terms in search\n")
                    f.write("  4. Add local language searches\n\n")

                if pre_analysis['geographic_relevance'] < 5:
                    f.write("TO IMPROVE GEOGRAPHIC RELEVANCE:\n")
                    f.write("  1. Verify destination parsing is correct\n")
                    f.write("  2. Include neighborhood names in search\n")
                    f.write("  3. Target local tourism/food sites\n\n")

                if pre_analysis['avg_quality'] < 6:
                    f.write("TO IMPROVE CONTENT QUALITY:\n")
                    f.write("  1. Target editorial content vs user-generated\n")
                    f.write("  2. Include professional food critics\n")
                    f.write("  3. Avoid aggregator sites\n\n")

                # =================================================================
                # PERFORMANCE METRICS
                # =================================================================
                f.write("PERFORMANCE METRICS\n")
                f.write("-" * 40 + "\n")
                f.write(f"Total Time: {timing_data['total_time']:.2f}s\n")
                f.write(f"Editor Time: {timing_data['step7_time']:.2f}s\n")
                f.write(f"Extraction Rate: {actual / max(len(cleaned_results), 1):.1f} restaurants/source\n")

                if timing_data['step7_time'] > 0:
                    chars_per_sec = pre_analysis['total_chars'] / timing_data['step7_time']
                    f.write(f"Processing Speed: {chars_per_sec:,.0f} chars/second\n")

                f.write("\n")

                # =================================================================
                # CONCLUSION
                # =================================================================
                f.write("CONCLUSION\n")
                f.write("-" * 40 + "\n")

                if actual == 5:
                    f.write("‚úÖ 5 RESTAURANTS IS LIKELY CORRECT\n")
                    f.write("The editor found exactly what was extractable from the content.\n")
                    f.write("This is quality-driven selection, not a hardcoded limit.\n\n")
                elif actual == 0:
                    f.write("‚ùå CONTENT QUALITY ISSUE\n")
                    f.write("The sources don't contain extractable restaurant information.\n")
                    f.write("Need better search targeting or different sources.\n\n")
                else:
                    f.write(f"üìä {actual} RESTAURANTS EXTRACTED\n")
                    f.write("Editor performance aligns with content quality.\n\n")

                f.write("‚úÖ TRANSPARENCY ANALYSIS COMPLETE\n")
                f.write("=" * 80 + "\n")

        except Exception as e:
            logger.error(f"‚ùå Error saving editor analysis: {e}")

    async def _send_results_to_admin(
        self, bot, scraped_filepath: str, edited_filepath: str, 
        query: str, editor_results: Dict[str, Any]
    ):
        """
        Send results to admin with summary
        """
        try:
            restaurants = editor_results.get('edited_results', {}).get('main_list', [])

            summary = (
                f"‚úèÔ∏è <b>Editor Pipeline Test Complete</b>\n\n"
                f"üìù Query: <code>{query}</code>\n"
                f"üçΩÔ∏è Restaurants Found: {len(restaurants)}\n\n"
                f"üìÑ <b>Files Generated:</b>\n"
                f"1. üìÑ Scraped Content Analysis\n"
                f"2. ‚úèÔ∏è Editor Decision Transparency\n\n"
                f"üéØ <b>Pipeline:</b> Web Search ‚Üí Scraping ‚Üí Text Cleaner ‚Üí Editor\n"
                f"üîç <b>Focus:</b> Why {len(restaurants)} restaurants were extracted"
            )

            bot.send_message(self.admin_chat_id, summary, parse_mode='HTML')

            # Send files
            with open(scraped_filepath, 'rb') as f:
                bot.send_document(self.admin_chat_id, f, caption=f"üìÑ Scraped Content: {query}")

            with open(edited_filepath, 'rb') as f:
                bot.send_document(self.admin_chat_id, f, caption=f"‚úèÔ∏è Editor Analysis: {query}")

            logger.info("‚úÖ Successfully sent transparent editor test results")

        except Exception as e:
            logger.error(f"‚ùå Failed to send results: {e}")


# =================================================================
# TELEGRAM BOT INTEGRATION
# =================================================================

def create_transparent_editor_test(config, orchestrator):
    """Factory function for creating the test"""
    return TransparentEditorTest(config, orchestrator)